{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import google.generativeai as genai\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import os\n",
    "import pyttsx3\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import time\n",
    "import threading\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "engine = pyttsx3.init()\n",
    "genai.configure(api_key=\"AIzaSyA13E1RS1Tz4fKTfozvzTtlWRguv4dQjkE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angle(a,b,c):\n",
    "    a = np.array(a) # Start\n",
    "    b = np.array(b) # Mid\n",
    "    c = np.array(c) # End\n",
    "    \n",
    "    radians = np.arctan2(c[1]-b[1], c[0]-b[0]) - np.arctan2(a[1]-b[1], a[0]-b[0])\n",
    "    angle = np.abs(radians*180.0/np.pi)\n",
    "    \n",
    "    if angle > 180.0:\n",
    "        angle = 360-angle\n",
    "        \n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech to Text and Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def speak_text(text):\n",
    "    engine.say(text)\n",
    "    engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feedback(exercise, reps, time):\n",
    "    prompt = f\"Stroke rehabilitation. I did {exercise} exercise: {reps} reps in {time:.2f} seconds. Provide short feedback.\"\n",
    "    model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
    "    response = model.generate_content(prompt)\n",
    "    print(\"Prompt: \" + prompt)\n",
    "    print(\"Feedback: \" + response.text)\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Arm Raise (Upper Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Counter, stage and time variables\n",
    "counter = 0\n",
    "stage = \"down\"\n",
    "start_time = None\n",
    "end_time = None\n",
    "\n",
    "# Intro instructions\n",
    "speak_text(\"This is the arm raise exercise tracker.\")\n",
    "time.sleep(1)\n",
    "speak_text(\"To perform the arm raise exercise, stand straight, keep your arms at your sides, and then raise both arms above your head, keeping your elbows straight.\")\n",
    "\n",
    "# Mediapipe pose detection\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read from the camera.\")\n",
    "            break\n",
    "        \n",
    "        # Convert frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "        \n",
    "        # Convert frame back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        \n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "            \n",
    "            # Get required landmarks\n",
    "            shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "            elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                     landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                   landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            left_heel = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "            right_heel = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "            nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "            left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y\n",
    "            right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y\n",
    "            \n",
    "            # Calculate angle at shoulder\n",
    "            angle = calculate_angle(elbow, shoulder, hip)\n",
    "            \n",
    "            # Display angle on screen\n",
    "            cv2.putText(image, str(int(angle)), \n",
    "                        tuple(np.multiply(shoulder, [640, 480]).astype(int)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Arm raise logic\n",
    "            if angle > 160 and stage == \"down\" and left_heel < 1 and right_heel < 1 and nose > 0 and left_wrist - 0.05 <= right_wrist <= left_wrist + 0.05:\n",
    "                stage = \"up\"\n",
    "                if counter == 0:\n",
    "                    start_time = time.time() \n",
    "                counter += 1\n",
    "                \n",
    "            if angle < 80 and stage == \"up\" and left_heel < 1 and right_heel < 1 and nose > 0 and left_wrist - 0.05 <= right_wrist <= left_wrist + 0.05:\n",
    "                stage = \"down\"\n",
    "                end_time = time.time()\n",
    "                threading.Thread(target=speak_text, args=(str(counter),)).start()\n",
    "                print(f\"Reps: {counter}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            pass\n",
    "        \n",
    "        # Display counter and stage\n",
    "        cv2.rectangle(image, (0, 0), (275, 75), (245, 117, 16), -1)\n",
    "        cv2.putText(image, str(counter) + \" \" + stage, \n",
    "                    (10, 60), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Draw landmarks\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "        \n",
    "        # Show the feed\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_time and end_time:\n",
    "    total_time = end_time - start_time\n",
    "    speak_text(get_feedback(\"Arm Raise (Upper Body)\", counter, total_time))\n",
    "else:\n",
    "    speak_text(get_feedback(\"Arm Raise (Upper Body)\", counter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Confusion Matrix:\n",
      "[[52  3]\n",
      " [ 8 43]]\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Incorrect Form   0.866667  0.945455  0.904348        55\n",
      "  Correct Form   0.934783  0.843137  0.886598        51\n",
      "\n",
      "      accuracy                       0.896226       106\n",
      "     macro avg   0.900725  0.894296  0.895473       106\n",
      "  weighted avg   0.899439  0.896226  0.895808       106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths for video folders\n",
    "correct_form_path = \"Testing Videos/Arm Raise Correct\"\n",
    "incorrect_form_path = \"Testing Videos/Arm Raise Incorrect\"\n",
    "\n",
    "\n",
    "# Metrics for confusion matrix\n",
    "y_true = []  # Actual labels\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "def process_video(video_path):\n",
    "    # Counter and stage variables\n",
    "    global flag, stage\n",
    "    flag = False\n",
    "    stage = \"down\"\n",
    "\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Mediapipe pose detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "\n",
    "            # Convert frame to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Convert frame back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                # Get required landmarks\n",
    "                shoulder = [landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y]\n",
    "                elbow = [landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].x,\n",
    "                         landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value].y]\n",
    "                hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                       landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                left_heel = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "                right_heel = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "                nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "                left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value].y\n",
    "                right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value].y\n",
    "\n",
    "                # Calculate angle at shoulder\n",
    "                angle = calculate_angle(elbow, shoulder, hip)\n",
    "\n",
    "                # Arm raise logic\n",
    "                if angle > 160 and stage == \"down\" and left_wrist - 0.05 <= right_wrist <= left_wrist + 0.05:\n",
    "                    stage = \"up\"\n",
    "                    \n",
    "                if angle < 80 and stage == \"up\" and left_wrist - 0.05 <= right_wrist <= left_wrist + 0.05:\n",
    "                    stage = \"down\"\n",
    "                    flag = True\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        cap.release()\n",
    "    \n",
    "    return flag\n",
    "\n",
    "# Process correct form videos\n",
    "for video_file in os.listdir(correct_form_path):\n",
    "    video_path = os.path.join(correct_form_path, video_file)\n",
    "    reps = process_video(video_path)\n",
    "    y_true.append(1)  # Actual label: Positive (Correct Form)\n",
    "    y_pred.append(1 if flag else 0)  # Predicted label: Positive if reps detected\n",
    "\n",
    "# Process incorrect form videos\n",
    "for video_file in os.listdir(incorrect_form_path):\n",
    "    video_path = os.path.join(incorrect_form_path, video_file)\n",
    "    reps = process_video(video_path)\n",
    "    y_true.append(0)  # Actual label: Negative (Incorrect Form)\n",
    "    y_pred.append(1 if flag else 0)  # Predicted label: Positive if reps detected\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # Generate Confusion Matrix and Metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Incorrect Form\", \"Correct Form\"],digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Knee Extension (Lower Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Counter, stage and time variables\n",
    "counter = 0\n",
    "stage = \"down\"\n",
    "start_time = None\n",
    "end_time = None\n",
    "\n",
    "# Intro instructions\n",
    "speak_text(\"This is the knee extension exercise tracker.\")\n",
    "time.sleep(1)\n",
    "speak_text(\"To perform the knee extension exercise, sit on a chair, keep your feet flat on the ground, and then lift one leg, extending the knee fully while keeping the other leg bent.\")\n",
    "\n",
    "# Mediapipe pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Convert frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Convert frame back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Get required landmarks\n",
    "            hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].z,\n",
    "                   landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].z,\n",
    "                    landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "            ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].z,\n",
    "                      landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "            \n",
    "            hip_height = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y\n",
    "            knee_height = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y\n",
    "            left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y\n",
    "            right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y\n",
    "\n",
    "            left_heel = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "            right_heel = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "            nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "            \n",
    "            \n",
    "            kneeDisplay = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x,\n",
    "                    landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "\n",
    "            # Calculate angle at knee\n",
    "            angle = calculate_angle(hip, knee, ankle)\n",
    "\n",
    "            # Display angle on screen\n",
    "            cv2.putText(image, str(int(angle)), \n",
    "                        tuple(np.multiply(kneeDisplay, [640, 480]).astype(int)), \n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Knee extension logic\n",
    "            if angle > 170 and stage == \"down\" and knee_height - 0.15 <= hip_height <= knee_height + 0.15 and left_ankle - 0.1 <= right_ankle <= left_ankle + 0.1 and left_heel < 1 and right_heel < 1 and nose > 0:\n",
    "                stage = \"up\"\n",
    "                if counter == 0:\n",
    "                    start_time = time.time() \n",
    "                counter += 1\n",
    "            if angle < 100 and stage == \"up\" and knee_height - 0.15 <= hip_height <= knee_height + 0.15 and left_ankle - 0.1 <= right_ankle <= left_ankle + 0.1 and left_heel < 1 and right_heel < 1 and nose > 0:\n",
    "                stage = \"down\"\n",
    "                end_time = time.time()\n",
    "                threading.Thread(target=speak_text, args=(str(counter),)).start()\n",
    "                print(f\"Reps: {counter}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        # Display counter and stage\n",
    "        cv2.rectangle(image, (0, 0), (275, 75), (245, 117, 16), -1)\n",
    "        cv2.putText(image, str(counter) + \" \" + stage,\n",
    "                    (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw landmarks\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Show the feed\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_time and end_time:\n",
    "    total_time = end_time - start_time\n",
    "    speak_text(get_feedback(\"Knee Extension (Lower Body)\", counter, total_time))\n",
    "else:\n",
    "    speak_text(get_feedback(\"Knee Extension (Lower Body)\", counter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "'NoneType' object has no attribute 'landmark'\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Confusion Matrix:\n",
      "[[53  9]\n",
      " [14 30]]\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Incorrect Form   0.791045  0.854839  0.821705        62\n",
      "  Correct Form   0.769231  0.681818  0.722892        44\n",
      "\n",
      "      accuracy                       0.783019       106\n",
      "     macro avg   0.780138  0.768328  0.772298       106\n",
      "  weighted avg   0.781990  0.783019  0.780688       106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths for video folders\n",
    "correct_form_path = \"Testing Videos/Knee Extension Correct\"\n",
    "incorrect_form_path = \"Testing Videos/Knee Extension Incorrect\"\n",
    "\n",
    "# Metrics for confusion matrix\n",
    "y_true = []  # Actual labels\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "def process_video(video_path):\n",
    "    # Flag and stage variables\n",
    "    global flag, stage\n",
    "    flag = False\n",
    "    stage = \"down\"\n",
    "\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Mediapipe pose detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "\n",
    "            # Convert frame to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Convert frame back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                # Get required landmarks\n",
    "                hip = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].z,\n",
    "                   landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                knee = [landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].z,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y]\n",
    "                ankle = [landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].z,\n",
    "                        landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y]\n",
    "                \n",
    "                hip_height = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y\n",
    "                knee_height = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y\n",
    "                left_ankle = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y\n",
    "                right_ankle = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y\n",
    "\n",
    "                # Calculate angle at knee\n",
    "                angle = calculate_angle(hip, knee, ankle)\n",
    "\n",
    "                # Knee extension logic\n",
    "                if angle > 170 and stage == \"down\" and knee_height - 0.15 <= hip_height <= knee_height + 0.15 and left_ankle - 0.1 <= right_ankle <= left_ankle + 0.1:\n",
    "                    stage = \"up\"\n",
    "                if angle < 100 and stage == \"up\" and knee_height - 0.15 <= hip_height <= knee_height + 0.15 and left_ankle - 0.1 <= right_ankle <= left_ankle + 0.1:\n",
    "                    stage = \"down\"\n",
    "                    flag = True\n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "\n",
    "        cap.release()\n",
    "    \n",
    "    return flag\n",
    "\n",
    "# Process correct form videos\n",
    "for video_file in os.listdir(correct_form_path):\n",
    "    video_path = os.path.join(correct_form_path, video_file)\n",
    "    reps = process_video(video_path)\n",
    "    y_true.append(1)  # Actual label: Positive (Correct Form)\n",
    "    y_pred.append(1 if flag else 0)  # Predicted label: Positive if reps detected\n",
    "\n",
    "# Process incorrect form videos\n",
    "for video_file in os.listdir(incorrect_form_path):\n",
    "    video_path = os.path.join(incorrect_form_path, video_file)\n",
    "    reps = process_video(video_path)\n",
    "    y_true.append(0)  # Actual label: Negative (Incorrect Form)\n",
    "    y_pred.append(1 if flag else 0)  # Predicted label: Positive if reps detected\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Generate Confusion Matrix and Metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Incorrect Form\", \"Correct Form\"], digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sit To Stand (Full Body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Counter, stage, and time variables\n",
    "counter = 0\n",
    "stage = \"sit\"\n",
    "start_time = None\n",
    "end_time = None\n",
    "\n",
    "# Intro instructions\n",
    "speak_text(\"This is the sit-to-stand exercise tracker.\")\n",
    "time.sleep(1)\n",
    "speak_text(\"To perform the sit-to-stand exercise, sit on a chair, then rise to a standing position, and return to sitting.\")\n",
    "\n",
    "# Mediapipe pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.7, min_tracking_confidence=0.7) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Convert frame back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Extract pose landmarks\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Get required landmark heights\n",
    "            hip_height = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y\n",
    "            knee_height = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y\n",
    "            ankle_height = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y\n",
    "            right_ankle_height = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y\n",
    "            left_heel = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "            right_heel = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "            nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "\n",
    "            hip_display = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                           landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "            \n",
    "            # Calculate the ratio between hip to ankle and hip to knee heights\n",
    "            height_ratio = abs(hip_height - ankle_height) / abs(hip_height - knee_height)\n",
    "\n",
    "            cv2.putText(image, f\"Height Ratio: {height_ratio:.2f}\",\n",
    "                        tuple(np.multiply(hip_display, [640, 480]).astype(int)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "            # Sit-to-stand logic\n",
    "            if height_ratio < 2  and stage == \"sit\" and left_heel < 1 and right_heel < 1 and nose > 0 and ankle_height - 0.05 <= right_ankle_height <= ankle_height + 0.05 and hip_height <= knee_height - 0.08:\n",
    "                stage = \"stand\"\n",
    "                if counter == 0:\n",
    "                    start_time = time.time() \n",
    "                counter += 1\n",
    "\n",
    "            if height_ratio > 2.25 and stage == \"stand\" and left_heel < 1 and right_heel < 1 and nose > 0 and ankle_height - 0.05 <= right_ankle_height <= ankle_height + 0.05 and hip_height <= knee_height - 0.08:\n",
    "                stage = \"sit\"\n",
    "                end_time = time.time()\n",
    "                threading.Thread(target=speak_text, args=(str(counter),)).start()\n",
    "                print(f\"Reps: {counter}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # Display counter and stage\n",
    "        cv2.rectangle(image, (0, 0), (275, 75), (245, 117, 16), -1)\n",
    "        cv2.putText(image, f\"{counter} {stage}\",\n",
    "                    (10, 60),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 2, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw landmarks\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Show the feed\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if start_time and end_time:\n",
    "    total_time = end_time - start_time\n",
    "    speak_text(get_feedback(\"Sit To Stand (Full Body)\", counter, total_time))\n",
    "else:\n",
    "    speak_text(get_feedback(\"Sit To Stand (Full Body)\", counter, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\shaur\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\google\\protobuf\\symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0]\n",
      "Confusion Matrix:\n",
      "[[49 13]\n",
      " [ 2 77]]\n",
      "\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Incorrect Form   0.960784  0.790323  0.867257        62\n",
      "  Correct Form   0.855556  0.974684  0.911243        79\n",
      "\n",
      "      accuracy                       0.893617       141\n",
      "     macro avg   0.908170  0.882503  0.889250       141\n",
      "  weighted avg   0.901826  0.893617  0.891901       141\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paths for video folders\n",
    "correct_form_path = \"Testing Videos/Sit To Stand Correct\"\n",
    "incorrect_form_path = \"Testing Videos/Sit To Stand Incorrect\"\n",
    "\n",
    "# Metrics for confusion matrix\n",
    "y_true = []  # Actual labels\n",
    "y_pred = []  # Predicted labels\n",
    "\n",
    "def process_video(video_path):\n",
    "    # Flag and stage variables\n",
    "    global flag, stage\n",
    "    flag = False\n",
    "    stage = \"sit\"\n",
    "\n",
    "    # Load video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Mediapipe pose detection\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break  # End of video\n",
    "\n",
    "            # Convert frame to RGB\n",
    "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            image.flags.writeable = False\n",
    "            results = pose.process(image)\n",
    "\n",
    "            # Convert frame back to BGR\n",
    "            image.flags.writeable = True\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "            try:\n",
    "                landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "                # Get required landmarks\n",
    "                hip_height = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y\n",
    "                knee_height = landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y\n",
    "                ankle_height = landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y\n",
    "                right_ankle_height = landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y\n",
    "                left_heel = landmarks[mp_pose.PoseLandmark.LEFT_HEEL.value].y\n",
    "                right_heel = landmarks[mp_pose.PoseLandmark.RIGHT_HEEL.value].y\n",
    "                nose = landmarks[mp_pose.PoseLandmark.NOSE.value].y\n",
    "\n",
    "                hip_display = [landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x,\n",
    "                            landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y]\n",
    "                \n",
    "                # Calculate the ratio between hip to ankle and hip to knee heights\n",
    "                height_ratio = abs(hip_height - ankle_height) / abs(hip_height - knee_height)\n",
    "\n",
    "                cv2.putText(image, f\"Height Ratio: {height_ratio:.2f}\",\n",
    "                            tuple(np.multiply(hip_display, [640, 480]).astype(int)),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Sit-to-stand logic\n",
    "                if height_ratio < 2.00  and stage == \"sit\" and left_heel < 1 and right_heel < 1 and nose > 0 and ankle_height - 0.05 <= right_ankle_height <= ankle_height + 0.05 and hip_height <= knee_height - 0.08:\n",
    "                    stage = \"stand\"\n",
    "                    \n",
    "                if height_ratio > 2.25 and stage == \"stand\" and left_heel < 1 and right_heel < 1 and nose > 0 and ankle_height - 0.05 <= right_ankle_height <= ankle_height + 0.05 and hip_height <= knee_height - 0.08:\n",
    "                    stage = \"sit\"\n",
    "                    flag = True\n",
    "\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        cap.release()\n",
    "    \n",
    "    return flag\n",
    "\n",
    "# Process correct form videos\n",
    "for video_file in os.listdir(correct_form_path):\n",
    "    video_path = os.path.join(correct_form_path, video_file)\n",
    "    reps = process_video(video_path)\n",
    "    y_true.append(1)  # Actual label: Positive (Correct Form)\n",
    "    y_pred.append(1 if flag else 0)  # Predicted label: Positive if reps detected\n",
    "\n",
    "# Process incorrect form videos\n",
    "for video_file in os.listdir(incorrect_form_path):\n",
    "    video_path = os.path.join(incorrect_form_path, video_file)\n",
    "    reps = process_video(video_path)\n",
    "    y_true.append(0)  # Actual label: Negative (Incorrect Form)\n",
    "    y_pred.append(1 if flag else 0)  # Predicted label: Positive if reps detected\n",
    "\n",
    "print(y_true)\n",
    "print(y_pred)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# # Generate Confusion Matrix and Metrics\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, target_names=[\"Incorrect Form\", \"Correct Form\"], digits = 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Tracing Patterns (Balance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webcam input\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Counter, target variables, and timing\n",
    "counter = 0\n",
    "num_targets = 15  # Total number of targets\n",
    "target_position = [random.randint(100, 500), random.randint(100, 400)]  # Initial target position\n",
    "target_radius = 15  # Radius of the target\n",
    "move_interval = 0.5  # Time in seconds between movements\n",
    "last_move_time = time.time()\n",
    "\n",
    "# Check if a landmark is near the target\n",
    "def is_near_target(point, target, threshold=30):\n",
    "    distance = ((point[0] - target[0])**2 + (point[1] - target[1])**2)**0.5\n",
    "    return distance < threshold\n",
    "\n",
    "# Generate a random position for the target within the screen bounds\n",
    "def generate_random_position():\n",
    "    return [random.randint(100, 500), random.randint(100, 400)]\n",
    "\n",
    "# Intro instructions\n",
    "speak_text(\"This is the balance and coordination exercise tracker.\")\n",
    "time.sleep(1)\n",
    "speak_text(\"Trace the moving points on the screen using your hands or legs.\")\n",
    "\n",
    "# Mediapipe pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.7, min_tracking_confidence=0.7) as pose:\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Unable to read from the camera.\")\n",
    "            break\n",
    "\n",
    "        # Mirror the frame (flip horizontally)\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Convert frame to RGB\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        image.flags.writeable = False\n",
    "        results = pose.process(image)\n",
    "\n",
    "        # Convert frame back to BGR\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        try:\n",
    "            # Extract pose landmarks\n",
    "            landmarks = results.pose_landmarks.landmark\n",
    "\n",
    "            # Get positions for hand and foot\n",
    "            left_hand = [int(landmarks[mp_pose.PoseLandmark.LEFT_INDEX.value].x * 640),\n",
    "                         int(landmarks[mp_pose.PoseLandmark.LEFT_INDEX.value].y * 480)]\n",
    "            left_foot = [int(landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].x * 640),\n",
    "                         int(landmarks[mp_pose.PoseLandmark.LEFT_ANKLE.value].y * 480)]\n",
    "            right_hand = [int(landmarks[mp_pose.PoseLandmark.RIGHT_INDEX.value].x * 640),\n",
    "                          int(landmarks[mp_pose.PoseLandmark.RIGHT_INDEX.value].y * 480)]\n",
    "            right_foot = [int(landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].x * 640),\n",
    "                          int(landmarks[mp_pose.PoseLandmark.RIGHT_ANKLE.value].y * 480)]\n",
    "\n",
    "            # Draw the current target point\n",
    "            cv2.circle(image, tuple(target_position), target_radius, (0, 0, 255), -1)\n",
    "\n",
    "            # Check if a hand or foot is near the current target\n",
    "            if (is_near_target(left_hand, target_position) or\n",
    "                    is_near_target(left_foot, target_position) or\n",
    "                    is_near_target(right_hand, target_position) or\n",
    "                    is_near_target(right_foot, target_position)):\n",
    "                counter += 1\n",
    "                threading.Thread(target=speak_text, args=(f\"Target {counter} touched!\",)).start()\n",
    "                target_position = generate_random_position()  # Move to the next random position\n",
    "\n",
    "                # Check if the required number of targets has been hit\n",
    "                if counter >= num_targets:\n",
    "                    speak_text(\"Exercise complete!\")\n",
    "                    break\n",
    "\n",
    "            # Move the target periodically\n",
    "            if time.time() - last_move_time > move_interval:\n",
    "                target_position[0] += random.choice([-20, 20])\n",
    "                target_position[1] += random.choice([-20, 20])\n",
    "                # Keep target within screen bounds\n",
    "                target_position[0] = max(50, min(target_position[0], 590))\n",
    "                target_position[1] = max(50, min(target_position[1], 430))\n",
    "                last_move_time = time.time()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "\n",
    "        # Display counter\n",
    "        cv2.rectangle(image, (0, 0), (350, 75), (245, 117, 16), -1)\n",
    "        cv2.putText(image, f\"Targets Hit: {counter} / {num_targets}\",\n",
    "                    (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Draw landmarks\n",
    "        mp_drawing = mp.solutions.drawing_utils\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 117, 66), thickness=2, circle_radius=2),\n",
    "                                  mp_drawing.DrawingSpec(color=(245, 66, 230), thickness=2, circle_radius=2))\n",
    "\n",
    "        # Show the feed\n",
    "        cv2.imshow('Mediapipe Feed', image)\n",
    "\n",
    "        if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
